# Importing Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')
# Loading Data
data = pd.read_csv('/kaggle/input/term-life-insurance-targeting-high-value-customers/dataset.csv')
data.head()
age	job	marital	education_qual	call_type	day	mon	dur	num_calls	prev_outcome	
0	58	management	married	tertiary	unknown	5	may	261	1	unknown	no
1	44	technician	single	secondary	unknown	5	may	151	1	unknown	no
2	33	entrepreneur	married	secondary	unknown	5	may	76	1	unknown	no
3	47	blue-collar	married	unknown	        unknown	5	may	92	1	unknown	no
4	33	unknown	        single	unknown	        unknown	5	may	198	1	unknown	no
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 45211 entries, 0 to 45210
Data columns (total 11 columns):
 #   Column          Non-Null Count  Dtype 
---  ------          --------------  ----- 
 0   age             45211 non-null  int64 
 1   job             45211 non-null  object
 2   marital         45211 non-null  object
 3   education_qual  45211 non-null  object
 4   call_type       45211 non-null  object
 5   day             45211 non-null  int64 
 6   mon             45211 non-null  object
 7   dur             45211 non-null  int64 
 8   num_calls       45211 non-null  int64 
 9   prev_outcome    45211 non-null  object
 10  y               45211 non-null  object
dtypes: int64(4), object(7)
memory usage: 3.8+ MB
data.describe()
age	day	dur	num_calls
count	45211.000000	45211.000000	45211.000000	45211.000000
mean	40.936210	15.806419	258.163080	2.763841
std	10.618762	8.322476	257.527812	3.098021
min	18.000000	1.000000	0.000000	1.000000
25%	33.000000	8.000000	103.000000	1.000000
50%	39.000000	16.000000	180.000000	2.000000
75%	48.000000	21.000000	319.000000	3.000000
max	95.000000	31.000000	4918.000000	63.000000
data.shape
(45211, 11)
Dataset Dimensions
The dataset has 45,211 rows and 11 columns.
data.nunique()
age                 77
job                 12
marital              3
education_qual       4
call_type            3
day                 31
mon                 12
dur               1573
num_calls           48
prev_outcome         4
y                    2
dtype: int64
# Renaming columns to more meaningful names
column_mapping = {
    'age': 'customer_age',
    'job': 'job_type',
    'marital': 'marital_status',
    'education_qual': 'education_level',
    'call_type': 'contact_method',
    'day': 'call_day',
    'mon': 'call_month',
    'dur': 'call_duration',
    'num_calls': 'number_of_calls',
    'prev_outcome': 'previous_campaign_outcome',
    'y': 'subscription_status'
}

# Renaming the columns
data= data.rename(columns=column_mapping)

# Display the first few rows of the renamed dataset
data.head()
customer_age	job_type	marital_status	education_level	contact_method	call_day	call_month	call_duration	number_of_calls	previous_campaign_outcome	subscription_status
0	58	management	married	tertiary	unknown	5	may	261	1	unknown	no
1	44	technician	single	secondary	unknown	5	may	151	1	unknown	no
2	33	entrepreneur	married	secondary	unknown	5	may	76	1	unknown	no
3	47	blue-collar	married	unknown	unknown	5	may	92	1	unknown	no
4	33	unknown	single	unknown	unknown	5	may	198	1
data_encoded = data.drop('subscription_status', axis=1)
for column in data_encoded.select_dtypes(include=['object']).columns:
    data_encoded[column] = data_encoded[column].astype('category').cat.codes

# Calculate correlation matrix
corr_matrix = data_encoded.corr()

# Plot the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Dataset')
plt.show()
# Set up the figure
fig, axes = plt.subplots(2, 2, figsize=(16, 15))
fig.suptitle('Distribution of Various Features Before Replacement', fontsize=16)

# Plot 1: Distribution of Job Types
job_type_dist = data['job_type'].value_counts()
ax1 = sns.countplot(data=data, x='job_type', order=job_type_dist.index, ax=axes[0, 0])
ax1.set_title('Distribution of Job Types')
ax1.set_xlabel('Job Type')
ax1.set_ylabel('Count')
for p in ax1.patches:
    ax1.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')

# Plot 2: Distribution of Contact Methods
contact_method_dist = data['contact_method'].value_counts()
ax2 = sns.countplot(data=data, x='contact_method', order=contact_method_dist.index, ax=axes[0, 1])
ax2.set_title('Distribution of Contact Methods')
ax2.set_xlabel('Contact Method')
ax2.set_ylabel('Count')
for p in ax2.patches:
    ax2.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')

# Plot 3: Distribution of Education Levels
education_level_dist = data['education_level'].value_counts()
ax3 = sns.countplot(data=data, x='education_level', order=education_level_dist.index, ax=axes[1, 0])
ax3.set_title('Distribution of Education Levels')
ax3.set_xlabel('Education Level')
ax3.set_ylabel('Count')
for p in ax3.patches:
    ax3.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')

# Plot 4: Distribution of Previous Campaign Outcomes
prev_outcome_dist = data['previous_campaign_outcome'].value_counts()
ax4 = sns.countplot(data=data, x='previous_campaign_outcome', order=prev_outcome_dist.index, ax=axes[1, 1])
ax4.set_title('Distribution of Previous Campaign Outcomes')
ax4.set_xlabel('Previous Campaign Outcome')
ax4.set_ylabel('Count')
for p in ax4.patches:
    ax4.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')

# Adjust layout
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()
# Fill 'unknown' in 'job_type' column
most_common_job_type = data[data['job_type'] != 'unknown'].groupby(
    ['education_level'])['job_type'].agg(lambda x: x.value_counts().idxmax()).reset_index()

data = data.merge(
    most_common_job_type, 
    on='education_level', 
    how='left', 
    suffixes=('', '_common'))

data['job_type'] = data.apply(
    lambda row: row['job_type_common'] if row['job_type'] == 'unknown' else row['job_type'], 
    axis=1)

data.drop(columns=['job_type_common'], inplace=True)

# Fill 'unknown' in 'education_level' column
most_common_education_level = data[data['job_type'] != 'unknown'].groupby(
    [ 'job_type', 'marital_status'])['education_level'].agg(lambda x: x.value_counts().idxmax()).reset_index()

data = data.merge(
    most_common_education_level, 
    on=['job_type', 'marital_status'], 
    how='left', 
    suffixes=('', '_common'))

data['education_level'] = data.apply(
    lambda row: row['education_level_common'] if row['education_level'] == 'unknown' else row['education_level'], 
    axis=1)

data.drop(columns=['education_level_common'], inplace=True)

# Fill 'unknown' in 'contact_method' column
most_common_contact_method = data[data['contact_method'] != 'unknown'].groupby(
    ['call_month', 'previous_campaign_outcome'])['contact_method'].agg(lambda x: x.value_counts().idxmax()).reset_index()

data = data.merge(
    most_common_contact_method, 
    on=['call_month', 'previous_campaign_outcome'], 
    how='left', 
    suffixes=('', '_common'))

data['contact_method'] = data.apply(
    lambda row: row['contact_method_common'] if row['contact_method'] == 'unknown' else row['contact_method'], 
    axis=1)

data.drop(columns=['contact_method_common'], inplace=True)

# Fill 'unknown' in 'previous_campaign_outcome' column
most_common_prev_outcome = data[data['previous_campaign_outcome'] != 'unknown'].groupby(
    ['contact_method', 'number_of_calls'])['previous_campaign_outcome'].agg(lambda x: x.value_counts().idxmax()).reset_index()

data = data.merge(
    most_common_prev_outcome, 
    on=['contact_method', 'number_of_calls'], 
    how='left', 
    suffixes=('', '_common'))

data['previous_campaign_outcome'] = data.apply(
    lambda row: row['previous_campaign_outcome_common'] if row['previous_campaign_outcome'] == 'unknown' else row['previous_campaign_outcome'], 
    axis=1)

data.drop(columns=['previous_campaign_outcome_common'], inplace=True)

# Display the first few rows to verify the changes
data.head()
customer_age	job_type	marital_status	education_level	contact_method	call_day	call_month	call_duration	number_of_calls	previous_campaign_outcome	subscription_status
0	58	management	married	tertiary	cellular	5	may	261	1	failure	no
1	44	technician	single	secondary	cellular	5	may	151	1	failure	no
2	33	entrepreneur	married	secondary	cellular	5	may	76	1	failure	no
3	47	blue-collar	married	secondary	cellular	5	may	92	1	failure	no
4	33	blue-collar	single	secondary	cellular	5	may	198	1
# Set up the figure
fig, axes = plt.subplots(2, 2, figsize=(16, 15))
fig.suptitle('Distribution of Various Features After Replacement', fontsize=16)

# Plot 1: Distribution of Job Types
job_type_dist = data['job_type'].value_counts()
ax1 = sns.countplot(data=data, x='job_type', order=job_type_dist.index, ax=axes[0, 0])
ax1.set_title('Distribution of Job Types')
ax1.set_xlabel('Job Type')
ax1.set_ylabel('Count')
for p in ax1.patches:
    ax1.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')

# Plot 2: Distribution of Contact Methods
contact_method_dist = data['contact_method'].value_counts()
ax2 = sns.countplot(data=data, x='contact_method', order=contact_method_dist.index, ax=axes[0, 1])
ax2.set_title('Distribution of Contact Methods')
ax2.set_xlabel('Contact Method')
ax2.set_ylabel('Count')
for p in ax2.patches:
    ax2.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')

# Plot 3: Distribution of Education Levels
education_level_dist = data['education_level'].value_counts()
ax3 = sns.countplot(data=data, x='education_level', order=education_level_dist.index, ax=axes[1, 0])
ax3.set_title('Distribution of Education Levels')
ax3.set_xlabel('Education Level')
ax3.set_ylabel('Count')
for p in ax3.patches:
    ax3.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')

# Plot 4: Distribution of Previous Campaign Outcomes
prev_outcome_dist = data['previous_campaign_outcome'].value_counts()
ax4 = sns.countplot(data=data, x='previous_campaign_outcome', order=prev_outcome_dist.index, ax=axes[1, 1])
ax4.set_title('Distribution of Previous Campaign Outcomes')
ax4.set_xlabel('Previous Campaign Outcome')
ax4.set_ylabel('Count')
for p in ax4.patches:
    ax4.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                 ha='center', va='center', xytext=(0, 10), textcoords='offset points')

# Adjust layout
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()
X = data.drop('subscription_status', axis=1)  # Adjust column name as necessary
y = data['subscription_status']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess categorical features
categorical_features = ['job_type', 'marital_status', 'education_level', 'contact_method', 'call_month', 'previous_campaign_outcome']
numeric_features = ['customer_age', 'call_day', 'call_duration', 'number_of_calls']

# Define the preprocessing for numeric features (scaling)
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Define the preprocessing for categorical features (encoding)
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine the preprocessors
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Preprocess the training and testing data
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Initialize Logistic Regression model
log_reg = LogisticRegression(max_iter=1000)

# Train the model
log_reg.fit(X_train_resampled, y_train_resampled)

# Predict and evaluate
y_pred = log_reg.predict(X_test_processed)
y_prob = log_reg.predict_proba(X_test_processed)[:, 1]

log_reg_results = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred, pos_label='yes'),
    'Recall': recall_score(y_test, y_pred, pos_label='yes'),
    'F1 Score': f1_score(y_test, y_pred, pos_label='yes'),
    'ROC AUC': roc_auc_score(y_test, y_prob)
}

print("Logistic Regression Results:")
log_reg_results
Logistic Regression Results:
{'Accuracy': 0.8336835121088134,
 'Precision': 0.40345956054230947,
 'Recall': 0.7910174152153987,
 'F1 Score': 0.5343653250773993,
 'ROC AUC': 0.894610559784002}
from sklearn.tree import DecisionTreeClassifier

# Initialize Decision Tree model
decision_tree = DecisionTreeClassifier()

# Train the model
decision_tree.fit(X_train_resampled, y_train_resampled)

# Predict and evaluate
y_pred = decision_tree.predict(X_test_processed)
y_prob = decision_tree.predict_proba(X_test_processed)[:, 1]

decision_tree_results = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred, pos_label='yes'),
    'Recall': recall_score(y_test, y_pred, pos_label='yes'),
    'F1 Score': f1_score(y_test, y_pred, pos_label='yes'),
    'ROC AUC': roc_auc_score(y_test, y_prob)
}

print("Decision Tree Results:")
decision_tree_results
Decision Tree Results:
{'Accuracy': 0.8565741457480924,
 'Precision': 0.41965678627145087,
 'Recall': 0.4931255728689276,
 'F1 Score': 0.45343447113358615,
 'ROC AUC': 0.6997821023298361}
from sklearn.ensemble import RandomForestClassifier

# Initialize Random Forest model
random_forest = RandomForestClassifier(random_state=42)

# Train the model
random_forest.fit(X_train_resampled, y_train_resampled)

# Predict and evaluate
y_pred = random_forest.predict(X_test_processed)
y_prob = random_forest.predict_proba(X_test_processed)[:, 1]

random_forest_results = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred, pos_label='yes'),
    'Recall': recall_score(y_test, y_pred, pos_label='yes'),
    'F1 Score': f1_score(y_test, y_pred, pos_label='yes'),
    'ROC AUC': roc_auc_score(y_test, y_prob)
}

print("Random Forest Results:")
random_forest_results
Random Forest Results:
{'Accuracy': 0.8936193741015149,
 'Precision': 0.5629268292682926,
 'Recall': 0.5288725939505041,
 'F1 Score': 0.5453686200378071,
 'ROC AUC': 0.9038236637976346}
from sklearn.ensemble import GradientBoostingClassifier

# Initialize Gradient Boosting model
gradient_boosting = GradientBoostingClassifier()

# Train the model
gradient_boosting.fit(X_train_resampled, y_train_resampled)

# Predict and evaluate
y_pred = gradient_boosting.predict(X_test_processed)
y_prob = gradient_boosting.predict_proba(X_test_processed)[:, 1]

gradient_boosting_results = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred, pos_label='yes'),
    'Recall': recall_score(y_test, y_pred, pos_label='yes'),
    'F1 Score': f1_score(y_test, y_pred, pos_label='yes'),
    'ROC AUC': roc_auc_score(y_test, y_prob)
}

print("Gradient Boosting Results:")
gradient_boosting_results
Gradient Boosting Results:
{'Accuracy': 0.8701758266062147,
 'Precision': 0.47596988998262885,
 'Recall': 0.7534372135655362,
 'F1 Score': 0.5833924769339958,
 'ROC AUC': 0.9059344610283147}
# Store results in a dictionary
results = {
    'Logistic Regression': log_reg_results,
    'Decision Tree': decision_tree_results,
    'Random Forest': random_forest_results,
    'Gradient Boosting': gradient_boosting_results
}

# Convert results to DataFrame for better readability
results_df = pd.DataFrame(results).T

# Display the results table
results_df
Accuracy	Precision	Recall	F1 Score	ROC AUC
Logistic Regression	0.833684	0.403460	0.791017	0.534365	0.894611
Decision Tree	0.856574	0.419657	0.493126	0.453434	0.699782
Random Forest	0.893619	0.562927	0.528873	0.545369	0.903824
Gradient Boosting	0.870176	0.475970	0.753437	0.583392	0.905934
# Discretizing 'customer_age' into age groups
bins = [18, 30, 40, 50, 60, 70, 80, 95]
labels = ['18-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-95']
data['age_group'] = pd.cut(data['customer_age'], bins=bins, labels=labels)

# Analyze the subscription status by age group
age_group_analysis = data.groupby('age_group')['subscription_status'].value_counts(normalize=True).unstack()
age_group_analysis
subscription_status	no	yes
age_group		
18-30	0.837846	0.162154
31-40	0.897552	0.102448
41-50	0.909334	0.090666
51-60	0.899467	0.100533
61-70	0.594864	0.405136
71-80	0.548969	0.451031
81-95	0.565657	0.434343
# Analyzing subscription status by call day
call_day_analysis = data.groupby('call_day')['subscription_status'].value_counts(normalize=True).unstack()
call_day_analysis
subscription_status	no	yes
call_day		
1	0.720497	0.279503
2	0.859242	0.140758
3	0.835032	0.164968
4	0.840830	0.159170
5	0.887435	0.112565
6	0.906315	0.093685
7	0.913594	0.086406
8	0.890879	0.109121
9	0.885330	0.114670
10	0.769084	0.230916
11	0.877620	0.122380
12	0.847785	0.152215
13	0.847950	0.152050
14	0.886364	0.113636
15	0.860247	0.139753
16	0.864311	0.135689
17	0.909232	0.090768
18	0.901213	0.098787
19	0.930563	0.069437
20	0.930233	0.069767
21	0.900790	0.099210
22	0.829834	0.170166
23	0.865815	0.134185
24	0.861298	0.138702
25	0.841667	0.158333
26	0.887923	0.112077
27	0.866191	0.133809
28	0.921858	0.078142
29	0.926074	0.073926
30	0.826948	0.173052
31	0.928460	0.071540
# Analyzing subscription status by call month
call_month_analysis = data.groupby('call_month')['subscription_status'].value_counts(normalize=True).unstack()
call_month_analysis
subscription_status	no	yes
call_month		
apr	0.803206	0.196794
aug	0.889867	0.110133
dec	0.532710	0.467290
feb	0.833522	0.166478
jan	0.898788	0.101212
jul	0.909065	0.090935
jun	0.897772	0.102228
mar	0.480084	0.519916
may	0.932805	0.067195
nov	0.898489	0.101511
oct	0.562331	0.437669
sep	0.535406	0.464594
# Correlation between call_duration and subscription_status
# Binning 'call_duration' into discrete categories
call_duration_bins = [0, 60, 180, 300, 600, data['call_duration'].max()]
call_duration_labels = ['0-1 min', '1-3 min', '3-5 min', '5-10 min', '10+ min']
data['call_duration_group'] = pd.cut(data['call_duration'], bins=call_duration_bins, labels=call_duration_labels)

# Analyze the relationship between call duration group and subscription status
call_duration_analysis = data.groupby('call_duration_group')['subscription_status'].value_counts(normalize=True).unstack()
call_duration_analysis
subscription_status	no	yes
call_duration_group		
0-1 min	0.998110	0.001890
1-3 min	0.960881	0.039119
3-5 min	0.890824	0.109176
5-10 min	0.808463	0.191537
10+ min	0.516359	0.483641
# Visualize the relationship between call duration and subscription status
sns.boxplot(x='subscription_status', y='call_duration', data=data)
plt.title('Call Duration vs Subscription Status')
plt.show()
# Correlation between number_of_calls and subscription_status
# Binning 'number_of_calls' into discrete categories for better analysis
call_frequency_bins = [1, 2, 3, 4, 5, data['number_of_calls'].max()]
call_frequency_labels = ['1', '2', '3', '4', '5+']
data['call_frequency_group'] = pd.cut(data['number_of_calls'], bins=call_frequency_bins, labels=call_frequency_labels)

# Analyze the relationship between the number of calls and subscription status
call_frequency_analysis = data.groupby('call_frequency_group')['subscription_status'].value_counts(normalize=True).unstack()
call_frequency_analysis
subscription_status	no	yes
call_frequency_group		
1	0.887965	0.112035
2	0.888064	0.111936
3	0.909994	0.090006
4	0.921202	0.078798
5	0.941906	0.058094
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir("../input"))

# Any results you write to the current directory are saved as output.
['seed-from-uci', 'seeds-width-length', 'fishdata', 'wineuci', 'wikipediavectors']
ks=range(1,6)
inertias=[]

for k in ks:
    model=KMeans(n_clusters=k)
    model.fit(seeds)
    inertias.append(model.inertia_)

plt.plot(ks,inertias,'-o')
plt.xlabel('Number of Clusters , k')
plt.ylabel('Inertia')
plt.xticks(ks)
plt.show()

# Inertia decreases from 3 to 4 very slowly , so 3 can be a good choice
model=KMeans(n_clusters=3)
seed_labels=model.fit_predict(seeds)
centroids=model.cluster_centers_
centroids
array([[14.64847222, 14.46041667,  0.87916667,  5.56377778,  3.27790278,
         2.64893333,  5.19231944],
       [18.72180328, 16.29737705,  0.88508689,  6.20893443,  3.72267213,
         3.60359016,  6.06609836],
       [11.96441558, 13.27480519,  0.8522    ,  5.22928571,  2.87292208,
         4.75974026,  5.08851948]])
scaled_wine=scaler.fit_transform(wine)
scaled_wine.var(axis=0)
array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
scaled_wine=pd.DataFrame(scaled_wine)
scaled_wine.columns=['Alcohol','Malic_acid','Ash','Alcalinity_of_ash','Magnesium','Total_phenols','Flavanoids','Nonflavanoid_phenols','Proanthocyanins','Color_intensity','Hue','OD280','Proline']          
lables=KMeans(n_clusters=3).fit_predict(scaled_wine)
xs=scaled_wine.loc[:,'OD280']
ys=scaled_wine.loc[:,'Proline']
plt.scatter(xs,ys,c=labels)
<matplotlib.collections.PathCollection at 0x7f0537f072b0>